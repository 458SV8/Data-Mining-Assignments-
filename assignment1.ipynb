{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in X_train:\n",
      "177\n",
      "Infinite values in X_train:\n",
      "0\n",
      "\n",
      "NaN values in X_test:\n",
      "87\n",
      "Infinite values in X_test:\n",
      "0\n",
      "Model Evaluation:\n",
      "Logistic Regression: Mean Accuracy = 0.8272, Std = 0.0119\n",
      "Support Vector Machine: Mean Accuracy = 0.8316, Std = 0.0239\n",
      "K-Nearest Neighbors: Mean Accuracy = 0.8070, Std = 0.0313\n",
      "Decision Tree: Mean Accuracy = 0.7767, Std = 0.0318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shrey\\anaconda3\\python3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\Shrey\\anaconda3\\python3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\Shrey\\anaconda3\\python3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\Shrey\\anaconda3\\python3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\Shrey\\anaconda3\\python3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: Mean Accuracy = 0.8193, Std = 0.0416\n",
      "Naive Bayes: Mean Accuracy = 0.7654, Std = 0.0339\n",
      "Model Evaluation:\n",
      "Logistic Regression: Mean Accuracy = 82.60%, Std = 1.01%\n",
      "Support Vector Machine: Mean Accuracy = 82.83%, Std = 1.62%\n",
      "K-Nearest Neighbors: Mean Accuracy = 82.04%, Std = 2.18%\n",
      "Decision Tree: Mean Accuracy = 78.56%, Std = 1.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shrey\\anaconda3\\python3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\Shrey\\anaconda3\\python3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\Shrey\\anaconda3\\python3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\Shrey\\anaconda3\\python3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\Shrey\\anaconda3\\python3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: Mean Accuracy = 82.38%, Std = 1.00%\n",
      "Naive Bayes: Mean Accuracy = 77.89%, Std = 1.18%\n",
      "Linear SVC: Mean Accuracy = 82.94%, Std = 1.52%\n",
      "Perceptron: Mean Accuracy = 77.44%, Std = 3.78%\n",
      "Stochastic Gradient Descent: Mean Accuracy = 81.26%, Std = 1.83%\n",
      "\n",
      "Model Performance:\n",
      "                         Model      Score\n",
      "0                   Linear SVC  82.941435\n",
      "1       Support Vector Machine  82.827192\n",
      "2          Logistic Regression  82.603729\n",
      "3                Random Forest  82.378382\n",
      "4          K-Nearest Neighbors  82.039420\n",
      "5  Stochastic Gradient Descent  81.257297\n",
      "6                Decision Tree  78.564434\n",
      "7                  Naive Bayes  77.888394\n",
      "8                   Perceptron  77.444605\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "\n",
    "full_df = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "full_df['Title'] = full_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "\n",
    "title_mapping = {\n",
    "    \"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\",\n",
    "    \"Lady\": \"Royalty\", \"Countess\": \"Royalty\", \"Dona\": \"Royalty\",\n",
    "    \"Sir\": \"Royalty\", \"Don\": \"Royalty\", \"Jonkheer\": \"Royalty\",\n",
    "    \"Capt\": \"Officer\", \"Col\": \"Officer\", \"Major\": \"Officer\", \"Rev\": \"Officer\", \"Dr\": \"Officer\"\n",
    "}\n",
    "full_df['Title'] = full_df['Title'].replace(title_mapping)\n",
    "rare_titles = full_df['Title'].value_counts()[full_df['Title'].value_counts() < 10].index\n",
    "full_df['Title'] = full_df['Title'].replace(rare_titles, 'Rare')\n",
    "\n",
    "\n",
    "full_df['FamilySize'] = full_df['SibSp'] + full_df['Parch'] + 1\n",
    "\n",
    "\n",
    "full_df['IsAlone'] = 1  \n",
    "full_df.loc[full_df['FamilySize'] > 1, 'IsAlone'] = 0  \n",
    "\n",
    "\n",
    "full_df['Deck'] = full_df['Cabin'].str[0]\n",
    "full_df['Deck'] = full_df['Deck'].fillna('U')  \n",
    "\n",
    "\n",
    "full_df['FarePerPerson'] = full_df['Fare'] / full_df['FamilySize']\n",
    "\n",
    "\n",
    "full_df['Age*Class'] = full_df['Age'] * full_df['Pclass']\n",
    "\n",
    "\n",
    "full_df['Embarked'] = full_df['Embarked'].fillna(full_df['Embarked'].mode()[0])\n",
    "\n",
    "\n",
    "full_df['Fare'] = full_df['Fare'].fillna(full_df['Fare'].median())\n",
    "\n",
    "\n",
    "age_df = full_df[['Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Title']]\n",
    "\n",
    "age_df = pd.get_dummies(age_df, columns=['Sex', 'Title'], drop_first=True)\n",
    "\n",
    "known_age = age_df[age_df['Age'].notnull()]\n",
    "unknown_age = age_df[age_df['Age'].isnull()]\n",
    "\n",
    "X_train_age = known_age.drop('Age', axis=1)\n",
    "y_train_age = known_age['Age']\n",
    "X_test_age = unknown_age.drop('Age', axis=1)\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "rfr.fit(X_train_age, y_train_age)\n",
    "predicted_ages = rfr.predict(X_test_age)\n",
    "\n",
    "full_df.loc[full_df['Age'].isnull(), 'Age'] = predicted_ages\n",
    "\n",
    "scaler = StandardScaler()\n",
    "num_features = ['Age', 'Fare', 'FarePerPerson', 'FamilySize', 'Age*Class']\n",
    "full_df[num_features] = scaler.fit_transform(full_df[num_features])\n",
    "\n",
    "\n",
    "full_df = pd.get_dummies(full_df, columns=['Sex', 'Embarked', 'Title', 'Deck'], drop_first=True)\n",
    "\n",
    "\n",
    "drop_columns = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Parch']\n",
    "full_df.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "\n",
    "train_df_processed = full_df.iloc[:len(train_df)].copy()\n",
    "test_df_processed = full_df.iloc[len(train_df):].copy()\n",
    "\n",
    "train_df_processed.reset_index(drop=True, inplace=True)\n",
    "test_df_processed.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "y_train = train_df['Survived'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "X_train = train_df_processed.drop('Survived', axis=1)\n",
    "\n",
    "\n",
    "if 'Survived' in test_df_processed.columns:\n",
    "    test_df_processed.drop('Survived', axis=1, inplace=True)\n",
    "\n",
    "X_test = test_df_processed.copy()\n",
    "\n",
    "\n",
    "print(\"NaN values in X_train:\")\n",
    "print(X_train.isnull().sum().sum())\n",
    "\n",
    "print(\"Infinite values in X_train:\")\n",
    "print(np.isinf(X_train).sum().sum())\n",
    "\n",
    "\n",
    "if X_train.isnull().sum().sum() > 0 or np.isinf(X_train).sum().sum() > 0:\n",
    "    \n",
    "    X_train.fillna(0, inplace=True)\n",
    "    X_train.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "   \n",
    "\n",
    "print(\"\\nNaN values in X_test:\")\n",
    "print(X_test.isnull().sum().sum())\n",
    "\n",
    "print(\"Infinite values in X_test:\")\n",
    "print(np.isinf(X_test).sum().sum())\n",
    "\n",
    "\n",
    "if X_test.isnull().sum().sum() > 0 or np.isinf(X_test).sum().sum() > 0:\n",
    "    X_test.fillna(0, inplace=True)\n",
    "    X_test.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression(max_iter=1000)))\n",
    "models.append(('Support Vector Machine', SVC(probability=True)))\n",
    "models.append(('K-Nearest Neighbors', KNeighborsClassifier()))\n",
    "models.append(('Decision Tree', DecisionTreeClassifier()))\n",
    "models.append(('Random Forest', RandomForestClassifier(n_estimators=100)))\n",
    "models.append(('Naive Bayes', GaussianNB()))\n",
    "\n",
    "print(\"Model Evaluation:\")\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(f\"{name}: Mean Accuracy = {cv_results.mean():.4f}, Std = {cv_results.std():.4f}\")\n",
    "\n",
    "\n",
    "best_model = RandomForestClassifier(n_estimators=100)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': y_pred.astype(int)\n",
    "})\n",
    "\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression(max_iter=1000)))\n",
    "models.append(('Support Vector Machine', SVC(probability=True)))\n",
    "models.append(('K-Nearest Neighbors', KNeighborsClassifier()))\n",
    "models.append(('Decision Tree', DecisionTreeClassifier()))\n",
    "models.append(('Random Forest', RandomForestClassifier(n_estimators=100)))\n",
    "models.append(('Naive Bayes', GaussianNB()))\n",
    "models.append(('Linear SVC', SVC(kernel='linear', probability=True)))\n",
    "models.append(('Perceptron', Perceptron()))\n",
    "models.append(('Stochastic Gradient Descent', SGDClassifier()))\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "mean_scores = []\n",
    "\n",
    "print(\"Model Evaluation:\")\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    mean_score = cv_results.mean() * 100  # Convert to percentage\n",
    "    std_score = cv_results.std() * 100\n",
    "    mean_scores.append(mean_score)\n",
    "    names.append(name)\n",
    "    print(f\"{name}: Mean Accuracy = {mean_score:.2f}%, Std = {std_score:.2f}%\")\n",
    "\n",
    "\n",
    "model_performance = pd.DataFrame({\n",
    "    'Model': names,\n",
    "    'Score': mean_scores\n",
    "})\n",
    "\n",
    "model_performance.sort_values(by='Score', ascending=False, inplace=True)\n",
    "model_performance.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(model_performance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
